{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427dcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from torchvision.models import vgg16\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5ed845",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0625d5-fa60-4951-a49c-26e774b12e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd38d0b-8660-44ca-a8e0-1ed26a476970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = ImageFolder(root='Path to Train dataset', transform=transform)\n",
    "val_dataset = ImageFolder(root='Path to Val dataset', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=12, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=12, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a761f332-d58d-41fb-bfe9-ef5f9db1b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder: Truncated ConvNext-Base\n",
    "convnext_base = models.convnext_base(pretrained=True)\n",
    "encoder = nn.Sequential(*list(convnext_base.children())[:-2])  # Remove classifier layers\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "bottleneck_dim = 1024\n",
    "bottleneck = nn.Conv2d(1024, bottleneck_dim, kernel_size=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866aa35-5a8a-4ae4-b683-1c2d47861f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNextDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNextDecoder, self).__init__()\n",
    "        self.upsample_blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(1024, 768, kernel_size=4, stride=2, padding=1),\n",
    "                nn.LayerNorm([768, 16, 16]),\n",
    "                nn.GELU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(768, 512, kernel_size=4, stride=2, padding=1),\n",
    "                nn.LayerNorm([512, 32, 32]),\n",
    "                nn.GELU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "                nn.LayerNorm([256, 64, 64]),\n",
    "                nn.GELU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "                nn.LayerNorm([128, 128, 128]),\n",
    "                nn.GELU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "                nn.LayerNorm([64, 256, 256]),\n",
    "                nn.GELU()\n",
    "            )\n",
    "        ])\n",
    "        self.final_conv = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, encoder_features=None):\n",
    "        for idx, block in enumerate(self.upsample_blocks):\n",
    "            x = block(x)\n",
    "            if encoder_features is not None and idx < len(encoder_features):\n",
    "                x += encoder_features[-(idx+1)]\n",
    "        x = self.final_conv(x)\n",
    "        return self.tanh(x)\n",
    "        \n",
    "decoder = ConvNextDecoder().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f709df5-6f13-4a6b-b1a8-f31b62f07c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_encoder_features(encoder, images):\n",
    "    features = []\n",
    "    x = images\n",
    "    for layer in encoder.children():\n",
    "        x = layer(x)\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            features.append(x)\n",
    "    return features, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adf0879-f717-49de-88a7-7ebd52a098b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(bottleneck_dim, 768, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm([768, 8, 8]),\n",
    "\n",
    "            nn.Conv2d(768, 512, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm([512, 8, 8]),\n",
    "\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm([256, 8, 8]),\n",
    "\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm([128, 8, 8]),\n",
    "\n",
    "            nn.Dropout(0.3),  # Regularization\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm([64, 8, 8])\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, latent_vectors):\n",
    "        x = self.conv_layers(latent_vectors)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "discriminator = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0226f4cb-7d16-4616-a827-27ddf9afe9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('Reconstruct', exist_ok=True)\n",
    "\n",
    "def save_reconstructed_images(epoch, images, reconstructed_images):\n",
    "    save_path = f'Reconstruct/epoch_{epoch}_to_see.png'\n",
    "    comparison = torch.cat([images, reconstructed_images])\n",
    "    save_image(comparison, save_path, nrow=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b9450-465d-4380-b985-37bed81dac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss_fn = nn.MSELoss()\n",
    "adversarial_loss_fn = nn.BCELoss()\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr = 0.0001)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr = 0.0001)\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef243deb-2490-4193-9a40-f710d6fac96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples):\n",
    "    batch_size = real_samples.size(0)\n",
    "    alpha = torch.rand(batch_size, 1, 1, 1, device=real_samples.device)\n",
    "\n",
    "    interpolates = alpha * real_samples + (1 - alpha) * fake_samples\n",
    "    interpolates = interpolates.requires_grad_(True)\n",
    "\n",
    "    d_interpolates = discriminator(interpolates)\n",
    "\n",
    "    fake = torch.ones(d_interpolates.size(), device=real_samples.device)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "\n",
    "    gradients = gradients.reshape(batch_size, -1)\n",
    "\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c79ee-5f9a-44fc-b42f-15d36c3169aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(real_pred, fake_pred):\n",
    "    real_loss = torch.nn.functional.relu(1.0 - real_pred).mean()\n",
    "    fake_loss = torch.nn.functional.relu(1.0 + fake_pred).mean()\n",
    "    return real_loss + fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c8d37b-45fb-43d1-95ed-aaab66bc1f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "adv_weight_base = 0.0005\n",
    "gp_weight = 5\n",
    "n_critic = 5\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "counter = 0\n",
    "\n",
    "d_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(discriminator_optimizer, T_max=50)\n",
    "e_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(encoder_optimizer, T_max=50)\n",
    "\n",
    "# Integrate changes into the training loop\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    epoch_recon_loss = 0\n",
    "    epoch_encoder_adv_loss = 0\n",
    "    epoch_discriminator_loss = 0\n",
    "\n",
    "    real_mean = 0\n",
    "    fake_mean = 0\n",
    "    n_batches = 0\n",
    "\n",
    "    for images, _ in dataloader:\n",
    "        batch_size = images.size(0)\n",
    "        images = images.cuda()\n",
    "        n_batches += 1\n",
    "\n",
    "        for _ in range(n_critic):\n",
    "            discriminator.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                encoder_features, latent_vectors = extract_encoder_features(encoder, images)\n",
    "                latent_vectors = bottleneck(latent_vectors)\n",
    "            real_latent_vectors = torch.randn_like(latent_vectors).cuda()\n",
    "\n",
    "            noise = 0.01 * torch.randn_like(latent_vectors).cuda()\n",
    "            latent_vectors = latent_vectors + noise\n",
    "            real_latent_vectors = real_latent_vectors + noise\n",
    "\n",
    "            real_pred = discriminator(real_latent_vectors)\n",
    "            fake_pred = discriminator(latent_vectors.detach())\n",
    "\n",
    "            # Compute hinge loss and gradient penalty\n",
    "            d_loss = hinge_loss(real_pred, fake_pred)\n",
    "            gradient_penalty = compute_gradient_penalty(discriminator, real_latent_vectors, latent_vectors.detach())\n",
    "            d_loss = d_loss + gp_weight * gradient_penalty\n",
    "\n",
    "            d_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=0.5)\n",
    "            discriminator_optimizer.step()\n",
    "\n",
    "        # Train Encoder-Decoder\n",
    "        encoder.zero_grad()\n",
    "        decoder.zero_grad()\n",
    "\n",
    "        encoder_features, latent_vectors = extract_encoder_features(encoder, images)\n",
    "        latent_vectors = bottleneck(latent_vectors)\n",
    "        reconstructed_images = decoder(latent_vectors, encoder_features)\n",
    "        recon_loss = F.mse_loss(reconstructed_images, images)\n",
    "\n",
    "        fake_pred = discriminator(latent_vectors)\n",
    "        adv_weight = adv_weight_base * (epoch / num_epochs)  # Dynamic weighting\n",
    "        g_loss = -fake_pred.mean()\n",
    "\n",
    "        # Total loss prioritizing reconstruction\n",
    "        total_loss = recon_loss + adv_weight * g_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=0.5)\n",
    "        torch.nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=0.5)\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            real_mean += torch.sigmoid(real_pred).mean().item()\n",
    "            fake_mean += torch.sigmoid(fake_pred).mean().item()\n",
    "\n",
    "        epoch_recon_loss += recon_loss.item()\n",
    "        epoch_encoder_adv_loss += g_loss.item()\n",
    "        epoch_discriminator_loss += d_loss.item()\n",
    "\n",
    "    epoch_recon_loss /= n_batches\n",
    "    epoch_encoder_adv_loss /= n_batches\n",
    "    epoch_discriminator_loss /= n_batches\n",
    "    real_mean /= n_batches\n",
    "    fake_mean /= n_batches\n",
    "\n",
    "    d_scheduler.step()\n",
    "    e_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Recon Loss: {epoch_recon_loss:.4f} | \"\n",
    "          f\"Encoder Adv Loss: {epoch_encoder_adv_loss:.6f} | Discriminator Loss: {epoch_discriminator_loss:.4f}\")\n",
    "    print(f\"Real Mean: {real_mean:.4f}, Fake Mean: {fake_mean:.4f}\")\n",
    "\n",
    "\n",
    "    if real_mean < 0.6 or fake_mean > 0.6:\n",
    "        print(\"Warning: Discriminator may be overconfident!\")\n",
    "\n",
    "    # Validation step\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for val_images, _ in val_dataloader:\n",
    "            val_images = val_images.cuda()\n",
    "            val_features = encoder(val_images)\n",
    "            val_latent_vectors = bottleneck(val_features)\n",
    "            val_reconstructed_images = decoder(val_latent_vectors)\n",
    "\n",
    "            val_loss += F.mse_loss(val_reconstructed_images, val_images).item()\n",
    "\n",
    "        val_loss /= len(val_dataloader)\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "\n",
    "        torch.save({\n",
    "            'encoder': encoder,\n",
    "            'decoder': decoder,\n",
    "            'discriminator': discriminator,\n",
    "            'bottleneck': bottleneck\n",
    "        }, \"To save the complete AAE\")\n",
    "        print(f\"Models Saved...\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            sample_images, _ = next(iter(val_dataloader))\n",
    "            sample_images = sample_images.cuda()\n",
    "            features = encoder(sample_images)\n",
    "            latent_vectors = bottleneck(features)\n",
    "            reconstructed_samples = decoder(latent_vectors)\n",
    "            save_reconstructed_images(epoch, sample_images, reconstructed_samples)\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"Val loss didn't improve for {counter} epochs\")\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec547a2-4175-4c1e-b9b7-89776139b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"To save the complete AAE\", weights_only = False)\n",
    "encoder = checkpoint['encoder'].to(device)\n",
    "bottleneck = checkpoint['bottleneck'].to(device)\n",
    "decoder = checkpoint['decoder'].to(device)\n",
    "discriminator = checkpoint['discriminator'].to(device)\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "bottleneck.eval()\n",
    "discriminator.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_images, _ = next(iter(val_dataloader))\n",
    "    sample_images = sample_images.cuda()\n",
    "\n",
    "    features = encoder(sample_images)\n",
    "    latent_vectors = bottleneck(features)\n",
    "\n",
    "    # Add noise if required\n",
    "    noise = 0.01 * torch.randn_like(latent_vectors).cuda()\n",
    "    latent_vectors = latent_vectors + noise\n",
    "\n",
    "    reconstructed_samples = decoder(latent_vectors)\n",
    "    save_reconstructed_images(222, sample_images, reconstructed_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
